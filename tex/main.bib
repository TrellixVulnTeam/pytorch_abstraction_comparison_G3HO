@misc{paszke2019pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
    year = {2019},
    eprint = {1912.01703},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{abadi2016tensorflow,
    title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
    author = {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
    year = {2016},
    eprint = {1603.04467},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC}
}
@misc{chen2015mxnet,
    title = {MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
    author = {Tianqi Chen and Mu Li and Yutian Li and Min Lin and Naiyan Wang and Minjie Wang and Tianjun Xiao and Bing Xu and Chiyuan Zhang and Zheng Zhang},
    year = {2015},
    eprint = {1512.01274},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC}
}
@inproceedings{cntk,
    author = {Seide, Frank and Agarwal, Amit},
    title = {CNTK: Microsoft's Open-Source Deep-Learning Toolkit},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2945397},
    doi = {10.1145/2939672.2945397},
    abstract = {This tutorial will introduce the Computational Network Toolkit, or CNTK, Microsoft's cutting-edge open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. CNTK supports feed-forward, convolutional, and recurrent networks for speech, image, and text workloads, also in combination. Popular network types are supported either natively (convolution) or can be described as a CNTK configuration (LSTM, sequence-to-sequence). CNTK scales to multiple GPU servers and is designed around efficiency. The tutorial will give an overview of CNTK's general architecture and describe the specific methods and algorithms used for automatic differentiation, recurrent-loop inference and execution, memory sharing, on-the-fly randomization of large corpora, and multi-server parallelization. We will then show how typical uses looks like for relevant tasks like image recognition, sequence-to-sequence modeling, and speech recognition.},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {2135},
    numpages = {1},
    keywords = {neural networks, computational networks, neural network learning toolkit, CNTK, deep learning},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}
@misc{kaplan2020scaling,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@misc{you2017large,
    title = {Large Batch Training of Convolutional Networks},
    author = {Yang You and Igor Gitman and Boris Ginsburg},
    year = {2017},
    eprint = {1708.03888},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@misc{you2020large,
    title = {Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
    author = {Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
    year = {2020},
    eprint = {1904.00962},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@misc{abdelhamed2020ntire,
    title = {NTIRE 2020 Challenge on Real Image Denoising: Dataset, Methods and Results},
    author = {Abdelrahman Abdelhamed and Mahmoud Afifi and Radu Timofte and Michael S. Brown and Yue Cao and Zhilu Zhang and Wangmeng Zuo and Xiaoling Zhang and Jiye Liu and Wendong Chen and Changyuan Wen and Meng Liu and Shuailin Lv and Yunchao Zhang and Zhihong Pan and Baopu Li and Teng Xi and Yanwen Fan and Xiyu Yu and Gang Zhang and Jingtuo Liu and Junyu Han and Errui Ding and Songhyun Yu and Bumjun Park and Jechang Jeong and Shuai Liu and Ziyao Zong and Nan Nan and Chenghua Li and Zengli Yang and Long Bao and Shuangquan Wang and Dongwoon Bai and Jungwon Lee and Youngjung Kim and Kyeongha Rho and Changyeop Shin and Sungho Kim and Pengliang Tang and Yiyun Zhao and Yuqian Zhou and Yuchen Fan and Thomas Huang and Zhihao Li and Nisarg A. Shah and Wei Liu and Qiong Yan and Yuzhi Zhao and Marcin Możejko and Tomasz Latkowski and Lukasz Treszczotko and Michał Szafraniuk and Krzysztof Trojanowski and Yanhong Wu and Pablo Navarrete Michelini and Fengshuo Hu and Yunhua Lu and Sujin Kim and Wonjin Kim and Jaayeon Lee and Jang-Hwan Choi and Magauiya Zhussip and Azamat Khassenov and Jong Hyun Kim and Hwechul Cho and Priya Kansal and Sabari Nathan and Zhangyu Ye and Xiwen Lu and Yaqi Wu and Jiangxin Yang and Yanlong Cao and Siliang Tang and Yanpeng Cao and Matteo Maggioni and Ioannis Marras and Thomas Tanay and Gregory Slabaugh and Youliang Yan and Myungjoo Kang and Han-Soo Choi and Kyungmin Song and Shusong Xu and Xiaomu Lu and Tingniao Wang and Chunxia Lei and Bin Liu and Rajat Gupta and Vineet Kumar},
    year = {2020},
    eprint = {2005.04117},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@inproceedings{hall2020probability,
    title = {Probabilistic Object Detection: Definition and Evaluation},
    author = {David Hall and Feras Dayoub and John Skinner and Haoyang Zhang and Dimity Miller and Peter Corke and Gustavo Carneiro and Anelia Angelova and Niko Sünderhauf},
    booktitle = {IEEE Winter Conference on Applications of Computer Vision (WACV)},
    year = {2020}
}
@article{ILSVRC15,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = {{ImageNet Large Scale Visual Recognition Challenge}},
    Year = {2015},
    journal = {International Journal of Computer Vision (IJCV)},
    doi = {10.1007/s11263-015-0816-y},
    volume = {115},
    number = {3},
    pages = {211-252}
}
@misc{48051,
    title	 = {TensorFlow Graph Optimizations},
    author	 = {Rasmus Munk Larsen and Tatiana Shpeisman},
    year	 = {2019}
}
@INPROCEEDINGS{7780576,
    author = {W. {Shi} and J. {Caballero} and F. {Huszár} and J. {Totz} and A. P. {Aitken} and R. {Bishop} and D. {Rueckert} and Z. {Wang}},
    booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    title = {Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network},
    year = {2016},
    pages = {1874-1883},
    doi = {10.1109/CVPR.2016.207} }
@misc{brown2020language,
    title = {Language Models are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    eprint = {2005.14165},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}
@article{knuth,
    author = {Knuth, Donald E.},
    title = {Computer Programming as an Art},
    year = {1974},
    issue_date = {Dec 1974},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {17},
    number = {12},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/361604.361612},
    doi = {10.1145/361604.361612},
    abstract = {When Communications of the ACM began publication in 1959, the members of ACM's Editorial Board made the following remark as they described the purposes of ACM's periodicals [2]: “If computer programming is to become an important part of computer research and development, a transition of programming from an art to a disciplined science must be effected.” Such a goal has been a continually recurring theme during the ensuing years; for example, we read in 1970 of the “first steps toward transforming the art of programming into a science” [26]. Meanwhile we have actually succeeded in making our discipline a science, and in a remarkably simple way: merely by deciding to call it “computer science.”},
    journal = {Commun. ACM},
    month = dec,
    pages = {667–673},
    numpages = {7}
}
@book{10.5555/2935593,
    title = {Professional CUDA C Programming},
    year = {2014},
    isbn = {1118739329},
    publisher = {Wrox Press Ltd.},
    address = {GBR},
    edition = {1st},
    abstract = {Break into the powerful world of parallel GPU programmingwith this down-to-earth, practical guide Designed for professionals across multiple industrial sectors, Professional CUDA C Programming presents CUDA -- a parallel computing platform and programming model designed to ease the development of GPU programming -- fundamentals in an easy-to-follow format, and teaches readers how to think in parallel and implement parallel algorithms on GPUs. Each chapter covers a specific topic, and includes workable examples that demonstrate the development process, allowing readers to explore both the "hard" and "soft" aspects of GPU programming. Computing architectures are experiencing a fundamental shift toward scalable parallel computing motivated by application requirements in industry and science. This book demonstrates the challenges of efficiently utilizing compute resources at peak performance, presents modern techniques for tackling these challenges, while increasing accessibility for professionals who are not necessarily parallel programming experts. The CUDA programming model and tools empower developers to write high-performance applications on a scalable, parallel computing platform: the GPU. However, CUDA itself can be difficult to learn without extensive programming experience. Recognized CUDA authorities John Cheng, Max Grossman, and Ty McKercher guide readers through essential GPU programming skills and best practices in Professional CUDA C Programming, including: CUDA Programming Model GPU Execution Model GPU Memory model Streams, Event and Concurrency Multi-GPU Programming CUDA Domain-Specific Libraries Profiling and Performance Tuning The book makes complex CUDA concepts easy to understand for anyone with knowledge of basic software development with exercises designed to be both readable and high-performance. For the professional seeking entrance to parallel computing and the high-performance computing community, Professional CUDA C Programming is an invaluable resource, with the most current information available on the market.}
}
@ARTICLE{5751939,
    author = {C. M. {Wittenbrink} and E. {Kilgariff} and A. {Prabhu}},
    journal = {IEEE Micro},
    title = {Fermi GF100 GPU Architecture},
    year = {2011},
    volume = {31},
    number = {2},
    pages = {50-59},
    doi = {10.1109/MM.2011.24}
}
@ARTICLE{5009071,
    author = {M. J. {Flynn}},
    journal = {IEEE Transactions on Computers},
    title = {Some Computer Organizations and Their Effectiveness},
    year = {1972},
    volume = {C-21},
    number = {9},
    pages = {948-960},
    doi = {10.1109/TC.1972.5009071}
}
@misc{he2015deep,
    title = {Deep Residual Learning for Image Recognition},
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year = {2015},
    eprint = {1512.03385},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@inproceedings{Glaskowsky2009NVIDIAS,
    title = {NVIDIA's Fermi: The First Complete GPU Computing Architecture},
    author = {P. Glaskowsky},
    year = {2009}
}
@misc{cuda_toolkit,
    author = "{NVIDIA}",
    title = "CUDA Toolkit Documentation",
    year = "2020",
    howpublished = "\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#control-flow-instructions}",
    note = "[Online; accessed 3-December-2020]"
}
@misc{named_tensor,
    author = "{Alexander Rush}",
    title = "Tensor Considered Harmful",
    year = "2020",
    howpublished = "\url{http://nlp.seas.harvard.edu/NamedTensor}",
    note = "[Online; accessed 3-December-2020]"
}
@misc{vasilache2018tensor,
    title = {Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions},
    author = {Nicolas Vasilache and Oleksandr Zinenko and Theodoros Theodoridis and Priya Goyal and Zachary DeVito and William S. Moses and Sven Verdoolaege and Andrew Adams and Albert Cohen},
    year = {2018},
    eprint = {1802.04730},
    archivePrefix = {arXiv},
    primaryClass = {cs.PL}
}
@article{Chen_2017,
    title = {Typesafe abstractions for tensor operations (short paper)},
    ISBN = {9781450355292},
    url = {http://dx.doi.org/10.1145/3136000.3136001},
    DOI = {10.1145/3136000.3136001},
    journal = {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala - SCALA 2017},
    publisher = {ACM Press},
    author = {Chen, Tongfei},
    year = {2017}
}
@misc{le2019tflms,
    title = {TFLMS: Large Model Support in TensorFlow by Graph Rewriting},
    author = {Tung D. Le and Haruki Imai and Yasushi Negishi and Kiyokuni Kawachiya},
    year = {2019},
    eprint = {1807.02037},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@inproceedings{Pradelle2017PolyhedralOO,
    title = {Polyhedral Optimization of TensorFlow Computation Graphs},
    author = {B. Pradelle and Beno {\^i} t Meister and M. Baskaran and J. Springer and R. Lethin},
    booktitle = {ESPT/VPA@SC},
    year = {2017}
}
@misc{lattner2020mlir,
    title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
    author = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
    year = {2020},
    eprint = {2002.11054},
    archivePrefix = {arXiv},
    primaryClass = {cs.PL}
}
@misc{zerrell2019stripe,
    title = {Stripe: Tensor Compilation via the Nested Polyhedral Model},
    author = {Tim Zerrell and Jeremy Bruestle},
    year = {2019},
    eprint = {1903.06498},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC}
}
@INPROCEEDINGS{Griebl98codegeneration,
    author = {Martin Griebl and Christian Lengauer and Sabine Wetzel},
    title = {Code Generation in the Polytope Model},
    booktitle = {In IEEE PACT},
    year = {1998},
    pages = {106--111},
    publisher = {IEEE Computer Society Press}
}
@inproceedings{10.5555/3291168.3291211,
    author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
    title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
    year = {2018},
    isbn = {9781931971478},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms - such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
    booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
    pages = {579–594},
    numpages = {16},
    location = {Carlsbad, CA, USA},
    series = {OSDI'18}
}
@misc{he2015delving,
    title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year = {2015},
    eprint = {1502.01852},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}