@misc{paszke2019pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
    year = {2019},
    eprint = {1912.01703},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{abadi2016tensorflow,
    title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
    author = {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
    year = {2016},
    eprint = {1603.04467},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC}
}
@misc{chen2015mxnet,
    title = {MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
    author = {Tianqi Chen and Mu Li and Yutian Li and Min Lin and Naiyan Wang and Minjie Wang and Tianjun Xiao and Bing Xu and Chiyuan Zhang and Zheng Zhang},
    year = {2015},
    eprint = {1512.01274},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC}
}
@inproceedings{cntk,
    author = {Seide, Frank and Agarwal, Amit},
    title = {CNTK: Microsoft's Open-Source Deep-Learning Toolkit},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2945397},
    doi = {10.1145/2939672.2945397},
    abstract = {This tutorial will introduce the Computational Network Toolkit, or CNTK, Microsoft's cutting-edge open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. CNTK supports feed-forward, convolutional, and recurrent networks for speech, image, and text workloads, also in combination. Popular network types are supported either natively (convolution) or can be described as a CNTK configuration (LSTM, sequence-to-sequence). CNTK scales to multiple GPU servers and is designed around efficiency. The tutorial will give an overview of CNTK's general architecture and describe the specific methods and algorithms used for automatic differentiation, recurrent-loop inference and execution, memory sharing, on-the-fly randomization of large corpora, and multi-server parallelization. We will then show how typical uses looks like for relevant tasks like image recognition, sequence-to-sequence modeling, and speech recognition.},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {2135},
    numpages = {1},
    keywords = {neural networks, computational networks, neural network learning toolkit, CNTK, deep learning},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}
@misc{kaplan2020scaling,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@misc{you2017large,
    title = {Large Batch Training of Convolutional Networks},
    author = {Yang You and Igor Gitman and Boris Ginsburg},
    year = {2017},
    eprint = {1708.03888},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@misc{you2020large,
    title = {Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
    author = {Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
    year = {2020},
    eprint = {1904.00962},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@misc{abdelhamed2020ntire,
    title = {NTIRE 2020 Challenge on Real Image Denoising: Dataset, Methods and Results},
    author = {Abdelrahman Abdelhamed and Mahmoud Afifi and Radu Timofte and Michael S. Brown and Yue Cao and Zhilu Zhang and Wangmeng Zuo and Xiaoling Zhang and Jiye Liu and Wendong Chen and Changyuan Wen and Meng Liu and Shuailin Lv and Yunchao Zhang and Zhihong Pan and Baopu Li and Teng Xi and Yanwen Fan and Xiyu Yu and Gang Zhang and Jingtuo Liu and Junyu Han and Errui Ding and Songhyun Yu and Bumjun Park and Jechang Jeong and Shuai Liu and Ziyao Zong and Nan Nan and Chenghua Li and Zengli Yang and Long Bao and Shuangquan Wang and Dongwoon Bai and Jungwon Lee and Youngjung Kim and Kyeongha Rho and Changyeop Shin and Sungho Kim and Pengliang Tang and Yiyun Zhao and Yuqian Zhou and Yuchen Fan and Thomas Huang and Zhihao Li and Nisarg A. Shah and Wei Liu and Qiong Yan and Yuzhi Zhao and Marcin Możejko and Tomasz Latkowski and Lukasz Treszczotko and Michał Szafraniuk and Krzysztof Trojanowski and Yanhong Wu and Pablo Navarrete Michelini and Fengshuo Hu and Yunhua Lu and Sujin Kim and Wonjin Kim and Jaayeon Lee and Jang-Hwan Choi and Magauiya Zhussip and Azamat Khassenov and Jong Hyun Kim and Hwechul Cho and Priya Kansal and Sabari Nathan and Zhangyu Ye and Xiwen Lu and Yaqi Wu and Jiangxin Yang and Yanlong Cao and Siliang Tang and Yanpeng Cao and Matteo Maggioni and Ioannis Marras and Thomas Tanay and Gregory Slabaugh and Youliang Yan and Myungjoo Kang and Han-Soo Choi and Kyungmin Song and Shusong Xu and Xiaomu Lu and Tingniao Wang and Chunxia Lei and Bin Liu and Rajat Gupta and Vineet Kumar},
    year = {2020},
    eprint = {2005.04117},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@inproceedings{hall2020probability,
    title = {Probabilistic Object Detection: Definition and Evaluation},
    author = {David Hall and Feras Dayoub and John Skinner and Haoyang Zhang and Dimity Miller and Peter Corke and Gustavo Carneiro and Anelia Angelova and Niko Sünderhauf},
    booktitle = {IEEE Winter Conference on Applications of Computer Vision (WACV)},
    year = {2020}
}
@article{ILSVRC15,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = {{ImageNet Large Scale Visual Recognition Challenge}},
    Year = {2015},
    journal = {International Journal of Computer Vision (IJCV)},
    doi = {10.1007/s11263-015-0816-y},
    volume = {115},
    number = {3},
    pages = {211-252}
}
@misc{48051,
    title	 = {TensorFlow Graph Optimizations},
    author	 = {Rasmus Munk Larsen and Tatiana Shpeisman},
    year	 = {2019}
}
@INPROCEEDINGS{7780576,
    author = {W. {Shi} and J. {Caballero} and F. {Huszár} and J. {Totz} and A. P. {Aitken} and R. {Bishop} and D. {Rueckert} and Z. {Wang}},
    booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    title = {Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network},
    year = {2016},
    pages = {1874-1883},
    doi = {10.1109/CVPR.2016.207} }
@misc{brown2020language,
    title = {Language Models are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    eprint = {2005.14165},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}
@article{knuth,
    author = {Knuth, Donald E.},
    title = {Computer Programming as an Art},
    year = {1974},
    issue_date = {Dec 1974},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {17},
    number = {12},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/361604.361612},
    doi = {10.1145/361604.361612},
    abstract = {When Communications of the ACM began publication in 1959, the members of ACM's Editorial Board made the following remark as they described the purposes of ACM's periodicals [2]: “If computer programming is to become an important part of computer research and development, a transition of programming from an art to a disciplined science must be effected.” Such a goal has been a continually recurring theme during the ensuing years; for example, we read in 1970 of the “first steps toward transforming the art of programming into a science” [26]. Meanwhile we have actually succeeded in making our discipline a science, and in a remarkably simple way: merely by deciding to call it “computer science.”},
    journal = {Commun. ACM},
    month = dec,
    pages = {667–673},
    numpages = {7}
}
@book{10.5555/2935593,
    title = {Professional CUDA C Programming},
    year = {2014},
    isbn = {1118739329},
    publisher = {Wrox Press Ltd.},
    address = {GBR},
    edition = {1st},
    abstract = {Break into the powerful world of parallel GPU programmingwith this down-to-earth, practical guide Designed for professionals across multiple industrial sectors, Professional CUDA C Programming presents CUDA -- a parallel computing platform and programming model designed to ease the development of GPU programming -- fundamentals in an easy-to-follow format, and teaches readers how to think in parallel and implement parallel algorithms on GPUs. Each chapter covers a specific topic, and includes workable examples that demonstrate the development process, allowing readers to explore both the "hard" and "soft" aspects of GPU programming. Computing architectures are experiencing a fundamental shift toward scalable parallel computing motivated by application requirements in industry and science. This book demonstrates the challenges of efficiently utilizing compute resources at peak performance, presents modern techniques for tackling these challenges, while increasing accessibility for professionals who are not necessarily parallel programming experts. The CUDA programming model and tools empower developers to write high-performance applications on a scalable, parallel computing platform: the GPU. However, CUDA itself can be difficult to learn without extensive programming experience. Recognized CUDA authorities John Cheng, Max Grossman, and Ty McKercher guide readers through essential GPU programming skills and best practices in Professional CUDA C Programming, including: CUDA Programming Model GPU Execution Model GPU Memory model Streams, Event and Concurrency Multi-GPU Programming CUDA Domain-Specific Libraries Profiling and Performance Tuning The book makes complex CUDA concepts easy to understand for anyone with knowledge of basic software development with exercises designed to be both readable and high-performance. For the professional seeking entrance to parallel computing and the high-performance computing community, Professional CUDA C Programming is an invaluable resource, with the most current information available on the market.}
}
@ARTICLE{5751939,
    author = {C. M. {Wittenbrink} and E. {Kilgariff} and A. {Prabhu}},
    journal = {IEEE Micro},
    title = {Fermi GF100 GPU Architecture},
    year = {2011},
    volume = {31},
    number = {2},
    pages = {50-59},
    doi = {10.1109/MM.2011.24}
}
@ARTICLE{5009071,
    author = {M. J. {Flynn}},
    journal = {IEEE Transactions on Computers},
    title = {Some Computer Organizations and Their Effectiveness},
    year = {1972},
    volume = {C-21},
    number = {9},
    pages = {948-960},
    doi = {10.1109/TC.1972.5009071}
}
@misc{he2015deep,
    title = {Deep Residual Learning for Image Recognition},
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year = {2015},
    eprint = {1512.03385},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@inproceedings{Glaskowsky2009NVIDIAS,
    title = {NVIDIA's Fermi: The First Complete GPU Computing Architecture},
    author = {P. Glaskowsky},
    year = {2009}
}
@misc{cuda_toolkit,
    author = "{NVIDIA}",
    title = "CUDA Toolkit Documentation",
    year = "2020",
    howpublished = "\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#control-flow-instructions}",
    note = "[Online; accessed 3-December-2020]"
}
@misc{named_tensor,
    author = "{Alexander Rush}",
    title = "Tensor Considered Harmful",
    year = "2020",
    howpublished = "\url{http://nlp.seas.harvard.edu/NamedTensor}",
    note = "[Online; accessed 3-December-2020]"
}
@misc{vasilache2018tensor,
    title = {Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions},
    author = {Nicolas Vasilache and Oleksandr Zinenko and Theodoros Theodoridis and Priya Goyal and Zachary DeVito and William S. Moses and Sven Verdoolaege and Andrew Adams and Albert Cohen},
    year = {2018},
    eprint = {1802.04730},
    archivePrefix = {arXiv},
    primaryClass = {cs.PL}
}
@article{Chen_2017,
    title = {Typesafe abstractions for tensor operations (short paper)},
    ISBN = {9781450355292},
    url = {http://dx.doi.org/10.1145/3136000.3136001},
    DOI = {10.1145/3136000.3136001},
    journal = {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala - SCALA 2017},
    publisher = {ACM Press},
    author = {Chen, Tongfei},
    year = {2017}
}
@misc{le2019tflms,
    title = {TFLMS: Large Model Support in TensorFlow by Graph Rewriting},
    author = {Tung D. Le and Haruki Imai and Yasushi Negishi and Kiyokuni Kawachiya},
    year = {2019},
    eprint = {1807.02037},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@inproceedings{Pradelle2017PolyhedralOO,
    title = {Polyhedral Optimization of TensorFlow Computation Graphs},
    author = {B. Pradelle and Beno {\^i} t Meister and M. Baskaran and J. Springer and R. Lethin},
    booktitle = {ESPT/VPA@SC},
    year = {2017}
}
@misc{lattner2020mlir,
    title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
    author = {Chris Lattner and Mehdi Amini and Uday Bondhugula and Albert Cohen and Andy Davis and Jacques Pienaar and River Riddle and Tatiana Shpeisman and Nicolas Vasilache and Oleksandr Zinenko},
    year = {2020},
    eprint = {2002.11054},
    archivePrefix = {arXiv},
    primaryClass = {cs.PL}
}
@misc{zerrell2019stripe,
    title = {Stripe: Tensor Compilation via the Nested Polyhedral Model},
    author = {Tim Zerrell and Jeremy Bruestle},
    year = {2019},
    eprint = {1903.06498},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC}
}
@INPROCEEDINGS{Griebl98codegeneration,
    author = {Martin Griebl and Christian Lengauer and Sabine Wetzel},
    title = {Code Generation in the Polytope Model},
    booktitle = {In IEEE PACT},
    year = {1998},
    pages = {106--111},
    publisher = {IEEE Computer Society Press}
}
@inproceedings{10.5555/3291168.3291211,
    author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
    title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
    year = {2018},
    isbn = {9781931971478},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms - such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
    booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
    pages = {579–594},
    numpages = {16},
    location = {Carlsbad, CA, USA},
    series = {OSDI'18}
}
@misc{he2015delving,
    title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year = {2015},
    eprint = {1502.01852},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@book{10.5555/1891996,
    author = {Sanders, Jason and Kandrot, Edward},
    title = {CUDA by Example: An Introduction to General-Purpose GPU Programming},
    year = {2010},
    isbn = {0131387685},
    publisher = {Addison-Wesley Professional},
    edition = {1st},
    abstract = { This book is required reading for anyone working with accelerator-based computing systems. From the Foreword by Jack Dongarra, University of Tennessee and Oak Ridge National Laboratory CUDA is a computing architecture designed to facilitate the development of parallel programs. In conjunction with a comprehensive software platform, the CUDA Architecture enables programmers to draw on the immense power of graphics processing units (GPUs) when building high-performance applications. GPUs, of course, have long been available for demanding graphics and game applications. CUDA now brings this valuable resource to programmers working on applications in other domains, including science, engineering, and finance. No knowledge of graphics programming is requiredjust the ability to program in a modestly extended version of C. CUDA by Example, written by two senior members of the CUDA software platform team, shows programmers how to employ this new technology. The authors introduce each area of CUDA development through working examples. After a concise introduction to the CUDA platform and architecture, as well as a quick-start guide to CUDA C, the book details the techniques and trade-offs associated with each key CUDA feature. Youll discover when to use each CUDA C extension and how to write CUDA software that delivers truly outstanding performance. Major topics covered include Parallel programming Thread cooperation Constant memory and events Texture memory Graphics interoperability Atomics Streams CUDA C on multiple GPUs Advanced atomics Additional CUDA resources All the CUDA software tools youll need are freely available for download from NVIDIA.http://developer.nvidia.com/object/cuda-by-example.html}
}
@article{Besard_2019,
    title = {Effective Extensible Programming: Unleashing Julia on GPUs},
    volume = {30},
    ISSN = {2161-9883},
    url = {http://dx.doi.org/10.1109/TPDS.2018.2872064},
    DOI = {10.1109/tpds.2018.2872064},
    number = {4},
    journal = {IEEE Transactions on Parallel and Distributed Systems},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    author = {Besard, Tim and Foket, Christophe and De Sutter, Bjorn},
    year = {2019},
    month = {Apr},
    pages = {827–841}
}
@book{Locke1689-LOCAEC-4,
    publisher = {Oxford University Press},
    title = {An Essay Concerning Human Understanding},
    author = {John Locke},
    year = {1689}
}
@book{Russell1937-RUSPOM-7,
    author = {Bertrand Russell},
    publisher = {Routledge},
    title = {Principles of Mathematics},
    year = {1937}
}

@article{abstraction,
    Abstract = {We characterize abstraction in computer science by first comparing the fundamental nature of computer science with that of its cousin mathematics. We consider their primary products, use of formalism, and abstraction objectives, and find that the two disciplines are sharply distinguished. Mathematics, being primarily concerned with developing inference structures, has information neglect as its abstraction objective. Computer science, being primarily concerned with developing interaction patterns, has information hiding as its abstraction objective. We show that abstraction through information hiding is a primary factor in computer science progress and success through an examination of the ubiquitous role of information hiding in programming languages, operating systems, network architecture, and design patterns.},
    Author = {Colburn, Timothy and Shute, Gary},
    Da = {2007/07/01},
    Date-Added = {2020-12-10 21:25:53 -0600},
    Date-Modified = {2020-12-10 21:25:53 -0600},
    Doi = {10.1007/s11023-007-9061-7},
    Id = {Colburn2007},
    Isbn = {1572-8641},
    Journal = {Minds and Machines},
    Number = {2},
    Pages = {169--184},
    Title = {Abstraction in Computer Science},
    Ty = {JOUR},
    Url = {https://doi.org/10.1007/s11023-007-9061-7},
    Volume = {17},
    Year = {2007},
    Bdsk-Url-1 = {https://doi.org/10.1007/s11023-007-9061-7} }
@article{10.1145/244795.244798,
    author = {Pippenger, Nicholas},
    title = {Pure versus Impure Lisp},
    year = {1997},
    issue_date = {March 1997},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {19},
    number = {2},
    issn = {0164-0925},
    url = {https://doi.org/10.1145/244795.244798},
    doi = {10.1145/244795.244798},
    journal = {ACM Trans. Program. Lang. Syst.},
    month = mar,
    pages = {223–238},
    numpages = {16},
    keywords = {schematology, online computation}
}
@book{10.5555/1410219,
    author = {Skiena, Steven S.},
    title = {The Algorithm Design Manual},
    year = {2008},
    isbn = {1848000693},
    publisher = {Springer Publishing Company, Incorporated},
    edition = {2nd},
    abstract = {....The most comprehensive guide to designing practical and efficient algorithms!.... The Algorithm Design Manual, Second Edition "...the book is an algorithm-implementation treasure trove, and putting all of these implementations in one place was no small feat. The list of implementations [and] extensive bibliography make the book an invaluable resource for everyone interested in the subject." --ACM Computing Reviews "It has all the right ingredients: rich contents, friendly, personal language, subtle humor, the right references, and a plethora of pointers to resources."-- P. Takis Metaxas, Wellesley College "This is the most approachable book on algorithms I have." -- Megan Squire, Elon University, USA This newly expanded and updated second edition of the best-selling classic continues to take the "mystery" out of designing algorithms, and analyzing their efficacy and efficiency. Expanding onthe first edition, the book now serves as the primary textbook of choice for algorithm design courses while maintaining its status as the premier practical reference guide to algorithms for programmers, researchers, and students. The reader-friendly Algorithm Design Manual provides straightforward access to combinatorial algorithms technology, stressing design over analysis. The first part, Techniques, provides accessible instructionon methods for designing and analyzing computer algorithms. The second part, Resources, is intended for browsing and reference, and comprises the catalog of algorithmic resources, implementations and an extensive bibliography. NEW to the second edition: Doubles the tutorial material and exercises over the first edition Provides full online support for lecturers, and a completely updated and improved website component with lecture slides, audio and video Contains a unique catalog identifying the 75 algorithmic problems that arise most often in practice, leading the reader down the right path to solve them Includes several NEW "war stories" relating experiences from real-world applications Provides up-to-date links leading to the very best algorithm implementations available in C, C++, and Java ADDITIONAL Learning Tools: Exercises include "job interview problems" from major software companies Highlighted take-home lesson boxes emphasize essential concepts Provides comprehensive references to both survey articles and the primary literature Exercises points to relevant programming contest challenge problems Many algorithms presented with actual code (written in C) as well as pseudo-code A full set of lecture slides and additional material available at www.algorist.com Written by a well-known algorithms researcher who received the IEEE Computer Science and Engineering Teaching Award, this new edition of The Algorithm Design Manual is an essential learning tool for students needing a solid grounding in algorithms, as well as a special text/reference for professionals who need an authoritative and insightful guide. Professor Skiena is also author of the popular Springer text, Programming Challenges: The Programming Contest Training Manual.}
}
@inproceedings{10.1007/978-3-642-28869-2_1,
    author = {Stroustrup, Bjarne},
    title = {Foundations of C++},
    year = {2012},
    isbn = {9783642288685},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-642-28869-2_1},
    doi = {10.1007/978-3-642-28869-2_1},
    abstract = {C++ is a large and complicated language. People get lost in details. However, to write good C++ you only need to understand a few fundamental techniques --- the rest is indeed details. This paper presents fundamental examples and explains the principles behind them. Among the issues touched upon are type safety, resource management, compile-time computation, error-handling, concurrency, performance, object-oriented programming, and generic programming. The presentation relies on and introduces a few features from the recent ISO C++ standard, C++11, that simplify the discussion of C++ fundamentals and modern style.},
    booktitle = {Proceedings of the 21st European Conference on Programming Languages and Systems},
    pages = {1–25},
    numpages = {25},
    keywords = {fundamental techniques, C++, programming style},
    location = {Tallinn, Estonia},
    series = {ESOP'12}
}
@inproceedings{10.5555/3314872.3314876,
    author = {Panchenko, Maksim and Auler, Rafael and Nell, Bill and Ottoni, Guilherme},
    title = {BOLT: A Practical Binary Optimizer for Data Centers and Beyond},
    year = {2019},
    isbn = {9781728114361},
    publisher = {IEEE Press},
    abstract = {Abstract — Performance optimization for large-scale applications has recently become more important as computation continues to move towards data centers. Data-center applications are generally very large and complex, which makes code layout an important optimization to improve their performance. This has motivated recent investigation of practical techniques to improve code layout at both compile time and link time. Although post-link optimizers had some success in the past, no recent work has explored their benefits in the context of modern data-center applications. In this paper, we present BOLT, an open-source post-link optimizer built on top of the LLVM framework. Utilizing sample-based profiling, BOLT boosts the performance of real-world applications even for highly optimized binaries built with both feedback-driven optimizations (FDO) and link-time optimizations (LTO). We demonstrate that post-link performance improvements are complementary to conventional compiler optimizations, even when the latter are done at a whole-program level and in the presence of profile information. We evaluated BOLT on both Facebook data-center workloads and open-source compilers. For data-center applications, BOLT achieves up to 7.0% performance speedups on top of profile-guided function reordering and LTO. For the GCC and Clang compilers, our evaluation shows that BOLT speeds up their binaries by up to 20.4% on top of FDO and LTO, and up to 52.1% if the binaries are built without FDO and LTO.},
    booktitle = {Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
    pages = {2–14},
    numpages = {13},
    location = {Washington, DC, USA},
    series = {CGO 2019}
}
@misc{zhang2019fixup,
    title = {Fixup Initialization: Residual Learning Without Normalization},
    author = {Hongyi Zhang and Yann N. Dauphin and Tengyu Ma},
    year = {2019},
    eprint = {1901.09321},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@misc{goyal2018accurate,
    title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
    author = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
    year = {2018},
    eprint = {1706.02677},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@book{abelson1996structure,
    title = {Structure and Interpretation of Computer Programs},
    author = {Abelson, H. and Sussman, G.J. and Sussman, J.},
    isbn = {9780262011532},
    lccn = {96017756},
    series = {Electrical engineering and computer science series},
    year = {1996},
    publisher = {MIT Press}
}

@incollection{CARDOSO201717,
    Abstract = {This chapter provides an overview of the main concepts and representative computer architectures for high-performance embedded computing systems. As the multicore and many-core trends are shaping the organization of computing devices on all computing domain spectrums, from high-performance computing (HPC) to embedded computing, this chapter briefly describes some of the common CPU architectures, including platforms containing multiple cores/processors. In addition, this chapter introduces heterogeneous architectures, including hardware accelerators (GPUs and FPGAs) and system-on-a-chip (SoC) reconfigurable devices. The chapter highlights the importance of the Amdahl's law and its implications when offloading computations to hardware accelerators. Lastly, this chapter describes power/energy and performance models.},
    Address = {Boston},
    Author = {Jo {\~a} o M.P. Cardoso and Jos {\'e} Gabriel F. Coutinho and Pedro C. Diniz},
    Booktitle = {Embedded Computing for High Performance},
    Doi = {https://doi.org/10.1016/B978-0-12-804189-5.00002-8},
    Editor = {Jo {\~a} o M.P. Cardoso and Jos {\'e} Gabriel F. Coutinho and Pedro C. Diniz},
    Isbn = {978-0-12-804189-5},
    Keywords = {Computer architectures, Hardware accelerators, FPGAs, GPUs, Profiling, Amdahl's law},
    Pages = {17 - 56},
    Publisher = {Morgan Kaufmann},
    Title = {Chapter 2 - High-performance embedded computing},
    Url = {http://www.sciencedirect.com/science/article/pii/B9780128041895000028},
    Year = {2017},
    Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/B9780128041895000028},
    Bdsk-Url-2 = {https://doi.org/10.1016/B978-0-12-804189-5.00002-8} }

@article{DBLP:journals/corr/RomanoIM16,
    author = {Yaniv Romano and
 John Isidoro and
 Peyman Milanfar},
    title = {{RAISR:} Rapid and Accurate Image Super Resolution},
    journal = {CoRR},
    volume = {abs/1606.01299},
    year = {2016},
    url = {http://arxiv.org/abs/1606.01299},
    archivePrefix = {arXiv},
    eprint = {1606.01299},
    timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
    biburl = {https://dblp.org/rec/journals/corr/RomanoIM16.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{cuda_toolkit,
    author = "{Computer Vision Machine Learning Team}",
    title = "An On-device Deep Neural Network for Face Detection",
    year = "2017",
    howpublished = "\url{https://machinelearning.apple.com/research/face-detection}",
    note = "[Online; accessed 3-December-2020]"
}
@article{DBLP:journals/corr/abs-1709-06429,
    author = {Shaona Ghosh and
 Per Ola Kristensson},
    title = {Neural Networks for Text Correction and Completion in Keyboard Decoding},
    journal = {CoRR},
    volume = {abs/1709.06429},
    year = {2017},
    url = {http://arxiv.org/abs/1709.06429},
    archivePrefix = {arXiv},
    eprint = {1709.06429},
    timestamp = {Mon, 13 Aug 2018 16:49:10 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1709-06429.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{oneapi,
    author = "{Intel}",
    title = "Driving a New Era of Accelerated Computing",
    year = "2020",
    howpublished = "\url{https://software.intel.com/content/www/us/en/develop/tools/oneapi.html}",
    note = "[Online; accessed 3-December-2020]"
}
@inproceedings{48690,
    title	 = {Generative Models for Effective ML on Private, Decentralized Datasets},
    author	 = {Sean Augenstein and Brendan McMahan and Daniel Ramage and Swaroop Ramaswamy and Peter Kairouz and Mingqing Chen and Rajiv Mathews and Blaise Aguera-Arcas},
    year	 = {2019},
    URL	 = {http://go/arxiv/1911.06679}
}

