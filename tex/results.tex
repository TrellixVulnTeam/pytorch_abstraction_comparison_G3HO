\subsection{Accuracy}\label{subsec:training-and-evaluation}
\input{figures/accuracy_results.tex}

The PyTorch implementation compares favorably with both LibTorch and the cuDNN implementations (see~\cref{fig:accuracy_results}).
On MNIST and CIFAR10 all three implementations perform reasonably well;
LibTorch and PyTorch attain maximum accuracy at around the same time while cuDNN lags behind.
On the more complex STL10 and PASCAL datasets (see~\cref{fig:other_accuracy_results} in the appendix) the cuDNN implementation dramatically underperformed PyTorch and LibTorch.
The cause of the difference between the cuDNN implementation and the others is unclear;
on the complex datasets the cuDNN implementation vacillated between vanishing gradients and exploding gradients.

In attempting to resolve the poor performance of the cuDNN implementation it was discovered that PyTorch (and LibTorch as well) initialize weights in convolutional, linear, and batch normalization layers.
This is \textbf{not documented and not configurable}.
For convolutional and linear layers Kaiming uniform initialization~\cite{he2015delving} is used and for batch normalization layers $(1,0)$ initialization for the weights and biases is used.
We implemented Kaiming initialization for the cuDNN implementation but it did not resolve the underperformance issues (the network vacillated between vanishing gradients and exploding gradients depending on various settings of the parameters in the Kaiming initialization).
Note that TorchScript training and evaluation accuracy is not measured/reported because TorchScript implementations cannot (as of yet) be trained, only evaluated.

The undocumented initialization leads us to believe that most likely there are several other heuristic optimizations implemented by Pytorch (and LibTorch).
While such optimizations generally do improve performance (to wit: here on STL10 and PASCAL) this prompts the question of whether or not this is a ``moral'' cost of abstraction (since the optimizations might hurt performance for particular models).

\subsection{Memory and utilization}\label{subsec:memory-and-utilization}
\input{figures/timing_results.tex}

In terms of execution time and memory usage PyTorch compares \textbf{unfavorably} with each of the other implementations.
We measure execution time, memory usage, and gpu utilization across training and evaluation on PASCAL for various batch sizes and resolution.
For example, for fixed batch size and various resolutions and for fixed resolution and various batch sizes (see~\cref{fig:timing_results}), we see that PyTorch is almost an order of magnitude slower than all other implementations for resolution $< 2^7$.
This execution time difference persists across resolutions and batch sizes but narrows as either increases (see~\cref{fig:train_avg_sample_time,,fig:train_avg_used_mem,,fig:train_avg_gpu_util,,fig:eval_avg_sample_time,,fig:eval_avg_used_mem,,fig:eval_avg_gpu_util} in the appendix).
With respect to memory usage PyTorch and LibTorch are approximately the same across resolutions and batch sizes, while cuDNN and TorchScript are more memory efficient especially below resolution $2^6$ and batch size $2^7$.

% TODO: discussion from nvvp