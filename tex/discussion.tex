% difficulty implementing cudnn (upfront costs vs pytorch)

Overall the central challenges for this work revolved around the cuDNN implementation.
Chief among them involved familiarizing ourselves with the SIMT compute model and the CUDA/cuDNN APIs.
It's important to contextualize these challenges appropriately: how much of the challenge is akin to the initial learning curve associated with any new software toolset (e.g.\ PyTorch) and how much is enduring.
Certainly memory leaks, and debugging them, given that C++ is not a memory managed language, will persist, but the difficulties associated with the idiosyncrasies of the APIs most likely won't.
Assuming that the majority of challenge decreases with time, are these lower levels of abstraction worth the investment of effort and time?

The lackluster accuracy results of cuDNN seemingly don't bode well for the hypothesis that one can, in a straightforward way, trade performance for implementation effort.
The cuDNN performance indicates there are serious bugs with the implementation.
Alternatively the accuracy results suggest that there are optimizations present in the PyTorch and LibTorch implementations that are obscured from the user (such as the Kaiming initialization mentioned in~\cref{sec:results}).
The former case, when juxtaposed with the execution time and memory usage results, suggests that the cuDNN implementation could be as accurate the PyTorch and LibTorch implementations, with much lower execution time and memory usage (assuming the bugs can be rectified \textemdash\ a reasonable assumption).
In the latter case, we face a sort of existential crisis: how many results in DL research, attributed to architectural innovations, in fact hinge on the implementation details of the frameworks those architectures are themselves implemented against.

In theory a sufficiently flexible DL compiler could rescue us from the purgatory we find ourselves in;
a sufficiently powerful compiler would implement the necessary DL abstractions in a robust way but also have enough flexibility to enable users to implement custom extensions of those abstractions.
One promising project that has as its goal such a high-level compiler is the ``Extensible Programming''~\cite{Besard_2019} project.
Besard et al. expose interfaces to alter the compilation process for Julia-lang\footnote{https://julialang.org/}.
The project instruments the Julia compiler itself and enables users to build high-level hardware abstractions in the source language itself%
\footnote{This is possible because Julia is JITed using LLVM and homo-iconic i.e. it supports a macrosystem that can manipulate the JITed LLVM bytecode.}.
They've had initial success writing high-level GPU code that performs comparably with CUDA C\footnote{https://github.com/JuliaGPU/CUDAnative.jl}.

