\documentclass[sigconf]{acmart}

\include{preamble}

\begin{document}

\title{When they go high we go low}

\author{Maksim Levental}
\authornote{Both authors contributed equally to this work.}
\email{mlevental@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}
\author{Elena Orlova}
\email{eorlova@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}

\renewcommand{\shortauthors}{Levental and Orlova}

\begin{abstract}
  High level abstractions for implementing, training, and testing Deep Learning (DL) models abound.
  Such frameworks function primarily by abstracting away the implementation details of arbitrary neural architectures, thereby enabling researchers and engineers to focus on design.
  In principle, such frameworks could be ``zero-cost abstractions'';
  in practice, they incur translation and indirection overheads.
  We study at which points exactly in the engineering life-cycle of a DL model the highest costs are paid and whether they can be mitigated.
  We train, test, and evaluate a representative DL model using PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets, comparing execution time and memory efficiency.
\end{abstract}


\maketitle

\section{Introduction}\label{sec:introduction}
\input{introduction}

\section{Background}\label{sec:background}
\input{background}

\section{Methods}\label{sec:methodology}
\input{methods}

\section{Results}\label{sec:results}
\input{results}

\section{Discussion}\label{sec:discussion}

\section{Future work}\label{sec:futurework}

\section{Speculation}\label{sec:speculation}

\bibliographystyle{science}
\bibliography{main}

\appendix
\appendixpage
\addappheadtotoc

\section{Appendix}\label{sec:appendix}

\input{figures/3dplots.tex}

\input{figures/other_accuracy_results.tex}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
