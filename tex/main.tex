\documentclass[sigconf]{acmart}
\usepackage{graphicx}

\include{preamble}

\begin{document}

\title{When they go high we go low}

\author{Maksim Levental}
\authornote{Both authors contributed equally to this work.}
\email{mlevental@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}
\author{Elena Orlova}
\email{eorlova@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}

\renewcommand{\shortauthors}{Levental and Orlova}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  High level abstractions for implementing, training, and testing Deep Learning (DL) models abound.
  Such frameworks function primarily by abstracting away the implementation details of arbitrary neural architectures, thereby enabling researchers and engineers to focus on design.
  In principle, such frameworks could be "zero-cost abstractions";
  in practice, they incur enormous translation and indirection overheads.
  We study at which points exactly in the engineering life-cycle of a DL model are the highest costs paid and whether they can be mitigated.
  We train, test, and evaluate a representative DL model using PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets.
\end{abstract}

%\include{plots/teaser}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:introduction}

Deep Learning (DL) frameworks represent neural network models as dataflow and computation graphs (where nodes correspond to functional units and edges correspond to composition).
In recent years, there has been a proliferation of DL frameworks~\cite{paszke2019pytorch,abadi2016tensorflow,chen2015mxnet,cntk} implemented as domain-specific languages (DSLs) embedded in "high-level" languages%
\footnote{For the purposes of this article, we take "high-level" to mean garbage collected and agnostic with respect to hardware from \textit{from the perspective of the user}.} such as Python, Java, and C\#.
These DSLs serve as \textit{abstractions} that aim to map the DL graphs onto hardware pipelines.
That is to say, they hide (or \textit{encapsulate}) details of DL models that are judged to be either irrelevant or too onerous to consider.
By virtue of these design decisions the frameworks trade-off ease-of-use for execution performance;
quoting the architects of PyTorch:
\begin{displayquote}
  To be useful, PyTorch needs to deliver compelling performance, although not at the expense of simplicity and ease of use.
  Trading 10\% of speed for a significantly simpler to use model is acceptable; 100\% is not.
\end{displayquote}

Trading off ergonomics for performance is manifestly reasonable%
\footnote{\textcquote{knuth}{The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.}}%
, especially during the early phases of the DL engineering/research process (i.e.\ during the hypothesis generation and experimentation phases).
Ultimately though, if one is in industry and taking for granted a research direction bears fruit, one needs to put the DL model into production.
It is at this phase of the DL engineering process that every percentage point of execution performance becomes critical.
Alternatively, there are many areas of academic DL where the research community strives to incrementally improve performance~\cite{abdelhamed2020ntire,hall2020probability,ILSVRC15}.
For example, in the area of super-resolution a deliberate goal is to be able to "super-resolve" in real-time~\cite{7780576}.
Similarly, in natural language processing, where enormous language models are becoming the norm~\cite{brown2020language}, memory efficiency of DL models is of the utmost concern.
In such instances it's natural to wonder whether ease-of-use trade-offs that sacrifice execution performance, or memory efficiency, are worthwhile and whether their costs can be mitigated.

Thus, our aim here is to investigate the costs of some of the abstractions employed by framework developers.
In particular we focus on the PyTorch framework and ecosystem (chosen for its popularity amongst academic researchers) on the software side and Graphics Processing Units (GPUs) on the hardware side.
To that end, we implement a popular and fairly representative%
\footnote{In the sense that the functional units constituting the model are widely used in various other models.}
DL model at four levels of abstraction: conventional PyTorch, LibTorch, cuDNN, and TorchScript.
We argue that in the forthcoming that these four implementations do in fact span considerable breadth in the abstraction spectrum.
Furthermore we train, test, evaluate each of the implementations on four object detection datasets and tabulate performance and accuracy metrics.

The rest of this article is organizing as follows: ~\cref{sec:background} quickly reviews the germaine background material on GPUs and DL frameworks, ~\cref{sec:methodology} describes the implementations and our profiling methodology,~\cref{sec:results} presents our results and a comparative discussion thereof, ~\cref{sec:discussion} discusses broad lessons learned, ~\cref{sec:futurework} proposes future work, and ~\cref{sec:speculation} speculates wildly about the future of DL systems more generally.

%the DL literature is rich in studies that investigate scaling DL models across various dimensions~\cite{kaplan2020scaling,you2017large,you2020large}

\section{Background}\label{sec:background}
\subsection{GPUs}\label{subsec:gpus}

We briefly review NVIDIA GPUs%
\footnote{A more comprehensive introduction to GPUs themselves and CUDA programming is available in~\cite{10.5555/2935593}.}
in order that the performance criteria we measure in~\cref{sec:methodology} are intelligible.

A GPU consists of many simple processors, called streaming multiprocessors (SMs), colloquially referred to as cores, that run at relatively low clock speeds%
\footnote{For example, individual NVIDIA GTX-1080 Ti cores run at $\sim$1500MHz.}
and execute one floating-point operation per clock cycle.
See ~\cref{fig:fermi_arch} for a diagram of NVIDIA's Fermi architecture, where each SM consists of 32 cores, 16 load/store (LD/ST) units, four special-function units (SFUs) which compute transcendental functions (such as $\sin$, $\cos$, $\exp$), a relatively large register file%
\footnote{For example, Intel's Haswell architecture supports 168 integer and 168 floating-point registers.}%
, and thread control logic (to be discussed in the proceeding).
Each SM has access to local memory, several cache levels, and global memory.
In the Fermi architecture (and subsequent architectures) this local memory is configurable in software;
a fraction of it can be apportioned as either local memory or L1 cache (for workloads that query global memory in excess of local memory).
One final feature worth mentioning, though irrelevant for us here, is the L2 cache's atomic read-modify-write facilities;
this enables shared data across groups of threads more efficiently than possible in conventional CPUs%
\footnote{On a CPU, atomic test-and-set instructions manage a semaphore which itself manages access to memory (therefore incurring a cost of at least two clock cycles).}.

Such an architecture, particularly suited to maximizing throughput, necessitates a programming model distinct from that of a conventional, general purpose processor architecture.
A unit of computation deployed to a GPU is called a \textit{kernel}; kernels can be defined using NVIDIA's Compute Unified Device Architecture (CUDA) extensions to C, C++, and FORTRAN%
\footnote{In fact CUDA compiles down to a virtual machine assembly code (by way of \code{nvcc}) for a virtual machine called the Parallel Thread Execution (PTX) virtual machine. So, in effect, it's compilers all the way down.}.
Compiled kernels are comprised by many \textit{threads} that execute the same instruction(s) in parallel;
NVIDIA describes this addition to Flynn's taxonomy~\cite{5009071} as Single Instruction Multiple Thread (SIMT).
The large register file enables very fast thread context switching ($\sim$25 microseconds on the Fermi architecture~\cite{Glaskowsky2009NVIDIAS}), performed by a centralized hardware thread scheduler.
Multiple threads are grouped into blocks (SMs are single tenant with respect to blocks) and blocks are grouped into \textit{grids} (grids execute a single kernel).
All threads in a block, by virtue of running on the same SM, coordinate (execute in arbitrary order, concurrently, or sequentially) and share memory.
Thread blocks are partitioned into \textit{warps} of 32 threads;
it is these warps that are dispatched by the warp scheduler (see~\cref{fig:cuda_cores}) and starting with the Fermi architecture two warps can be executed concurrently on the same SM in order to increase utilization.

\input{figures/fermi.tex}
\input{figures/cuda_code.tex}


\subsection{Graph compilers}\label{subsec:graph-compilers}

DL frameworks function as graph compilers.
What are the design choices made by DL framework architects and what are their costs?
That is to say, what are the costs of the abstractions

These frameworks encapsulate and "abstract away" many of the implementation details of DL, such as
\begin{itemize}
  \item building the dataflow graph between units/layers (and the corresponding gradient-flow graph)
  \item tensor manipulation and memory layout
  \item hardware specific optimizations
\end{itemize}

\subsubsection{Static}

\subsubsection{Dynamic}

\section{Methodology}\label{sec:methodology}

\begin{figure}
  \includegraphics[width=.7\linewidth]{figures/resnet50.png}
  \caption{ResNet-50 network architecture\cite{he2015deep}.}
  \label{fig:resnet}
\end{figure}

We implement ResNet-50 at four levels of abstraction: PyTorch, TorchScript, LibTorch, and cuDNN.
The reason for staying within the same ecosystem (PyTorch)is, in theory, we keep as many of the pieces of functionality orthogonal to our concerns as possible.
We'll see that that reasoning doesn't quite bear out (see\ref{sec:discussion}).

\subsection{Implementations}\label{subsec:implementations}

\subsection{Profiling}\label{subsec:profiling}

\section{Results}\label{sec:results}

\subsection{Training and evaluation}\label{subsec:training-and-evaluation}

\subsection{Memory and utilization}\label{subsec:memory-and-utilization}

\section{Discussion}\label{sec:discussion}

% initialization in conv layers in pytorch/libtorch
\section{Future work}\label{sec:futurework}
\section{Speculation}\label{sec:speculation}

%%% The next two lines define the bibliography style to be used, and
%%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\appendix
\appendixpage
\addappheadtotoc

\section{Appendix}\label{sec:appendix}

%\input{plots/3dplots.tex}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
