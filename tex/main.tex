\documentclass[sigconf]{acmart}
\usepackage{graphicx}

\include{preamble}

\begin{document}

\title{When they go high we go low}

\author{Maksim Levental}
\authornote{Both authors contributed equally to this research.}
\email{mlevental@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}
\author{Elena Orlova}
\email{eorlova@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}

\renewcommand{\shortauthors}{Levental and Orlova}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  High level abstractions for implementing, training, and testing Deep Learning (DL) models abound.
  Such frameworks function primarily by abstracting away the implementation details of arbitrary neural architectures, thereby enabling researchers and engineers to focus on design.
  In principle, such frameworks could be "zero-cost abstractions";
  in practice, they incur enormous translation and indirection overheads.
  We study at which points exactly in the engineering life-cycle of a DL model are the highest costs paid and whether they can be mitigated.
  We train, test, and evaluate a representative DL model implemented using PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets.
\end{abstract}

\include{plots/teaser}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:introduction}

In recent years, there has been a proliferation of Deep Learning (DL) frameworks\cite{paszke2019pytorch,abadi2016tensorflow,chen2015mxnet,cntk}.
Many of these frameworks are implemented as domain-specific languages embedded in "high-level" languages\footnote{For the purposes of this article, we take "high-level" to mean memory managed and agnostic with respect to hardware specifics.} such as Python, Java, and C\#.
By virtue of this design decision, the frameworks implicitly trade-off ease-of-use for performance; quoting the architects of PyTorch:
\begin{displayquote}
  To be useful, PyTorch needs to deliver compelling performance, although not at the expense of simplicity and ease of use.
  Trading 10\% of speed for a significantly simpler to use model is acceptable; 100\% is not.
\end{displayquote}
In general, this is a reasonable trade-off, especially during the early phases of the DL engineering/research process (i.e. during the hypothesis generation and experimentation phases).
Invariable though, taking for granted a research direction bears fruit, one needs to put into production the DL model.
It is at that phase of the process that every percentage point of performance becomes critical.
Alternatively, there are many areas of DL research where the community strives to incrementally improve performance\cite{abdelhamed2020ntire,hall2020probability,ILSVRC15}.
In such areas, where the ultimate goal is to build the most performant model, it's natural to wonder whether \textit{any} trade-off that sacrifices performance is worthwhile.

%the DL literature is rich in studies that investigate scaling DL models across various dimensions\cite{kaplan2020scaling,you2017large,you2020large}

\section{Background}\label{sec:problem}

\subsection{GPUs}\label{subsec:gpus}

\subsection{Graph compilers}\label{subsec:graph-compilers}

\subsubsection{Static}

\subsubsection{Dynamic}

\section{Methodology}\label{sec:context}

\begin{figure}
  \includegraphics[width=.7\linewidth]{plots/resnet50.png}\label{fig:figure5}
\end{figure}

We implement ResNet-50 at four levels of abstraction: PyTorch, TorchScript, LibTorch, and cuDNN.
The reason for staying within the same ecosystem (PyTorch)is, in theory, we keep as many of the pieces of functionality orthogonal to our concerns as possible.
We'll see that that reasoning doesn't quite bear out (see\ref{sec:discussion}).

\subsection{Implementations}\label{subsec:implementations}

\subsection{Profiling}\label{subsec:profiling}

\section{Results}\label{sec:approach}

\subsection{Training and evaluation}\label{subsec:training-and-evaluation}

\subsection{Memory and utilization}\label{subsec:memory-and-utilization}

\section{Discussion}\label{sec:results}

% initialization in conv layers in pytorch/libtorch
\section{Future work}\label{sec:discussion}
\section{Speculation}\label{sec:speculation}

%%% The next two lines define the bibliography style to be used, and
%%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\appendix
\appendixpage
\addappheadtotoc

\section{Appendix}\label{sec:appendix}

\input{plots/3dplots.tex}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
