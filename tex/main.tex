\documentclass[sigconf]{acmart}
\usepackage{graphicx}

\include{preamble}

\begin{document}

\title{When they go high we go low}

\author{Maksim Levental}
\authornote{Both authors contributed equally to this research.}
\email{mlevental@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}
\author{Elena Orlova}
\email{eorlova@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
}

\renewcommand{\shortauthors}{Levental and Orlova}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  High level abstractions for implementing, training, and testing Deep Learning (DL) models abound.
  Such frameworks function primarily by abstracting away the implementation details of arbitrary neural architectures, thereby enabling researchers and engineers to focus on design.
  In principle, such frameworks could be "zero-cost abstractions";
  in practice, they incur enormous translation and indirection overheads.
  We study at which points exactly in the engineering life-cycle of a DL model are the highest costs paid and whether they can be mitigated.
  We train, test, and evaluate a representative DL model using PyTorch, LibTorch, TorchScript, and cuDNN on representative datasets.
\end{abstract}

%\include{plots/teaser}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:introduction}

Deep Learning (DL) frameworks represent neural network models as dataflow and computation graphs (where nodes correspond to functional units and edges correspond to composition).
In recent years, there has been a proliferation of DL frameworks\cite{paszke2019pytorch,abadi2016tensorflow,chen2015mxnet,cntk} implemented as domain-specific languages (DSLs) embedded in "high-level" languages%
\footnote{For the purposes of this article, we take "high-level" to mean garbage collected and agnostic with respect to hardware from \textit{from the perspective of the user}.} such as Python, Java, and C\#.
These DSLs serve as \textit{abstractions} that aim to map the DL graphs onto hardware pipelines.
That is to say, they hide (or \textit{encapsulate}) details of DL models that are judged to be either irrelevant or too onerous to consider.
By virtue of these design decisions the frameworks trade-off ease-of-use for execution performance;
quoting the architects of PyTorch:
\begin{displayquote}
  To be useful, PyTorch needs to deliver compelling performance, although not at the expense of simplicity and ease of use.
  Trading 10\% of speed for a significantly simpler to use model is acceptable; 100\% is not.
\end{displayquote}

Trading off ergonomics for performance is manifestly reasonable%
\footnote{\textcquote{knuth}{The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.}}%
, especially during the early phases of the DL engineering/research process (i.e.\ during the hypothesis generation and experimentation phases).
Ultimately though, if one is in industry and taking for granted a research direction bears fruit, one needs to put the DL model into production.
It is at this phase of the DL engineering process that every percentage point of execution performance becomes critical.
Alternatively, there are many areas of academic DL where the research community strives to incrementally improve performance\cite{abdelhamed2020ntire,hall2020probability,ILSVRC15}.
For example, in the area of super-resolution a deliberate goal is to be able to "super-resolve" in real-time\cite{7780576}.
Similarly, in natural language processing, where enormous language models are becoming the norm\cite{brown2020language}, memory efficiency of DL models is of the utmost concern.
In such instances it's natural to wonder whether ease-of-use trade-offs that sacrifice execution performance, or memory efficiency, are worthwhile and whether their costs can be mitigated.

Thus, our aim here is to investigate the costs of some of the abstractions employed by framework developers.
In particular we focus on the PyTorch framework and ecosystem (chosen for its popularity amongst academic researchers).
To that end, we implement a popular and fairly representative%
\footnote{In the sense that the functional units constituting the model are widely used in various other models.}
DL model at four levels of abstraction: conventional Python/PyTorch, C++/LibTorch, C++/cuDNN, and C++/TorchScript.
We argue that in the forthcoming that these four implementations do in fact span considerable breadth in the abstraction spectrum.
Furthermore we train, test, evaluate each of the implementations on four object detection datasets and tabulate performance and accuracy metrics.

The rest of this article is organizing as follows: section\ref{sec:background} covers quickly reviews the germaine background material on graph compilers and GPUs, section\ref{sec:methodology} describes the implementations and our profiling methodology, section\ref{sec:results} presents our results and a comparative discussion thereof, section\ref{sec:discussion} discusses broad lessons learned, section\ref{sec:futurework} proposes future work, and section\ref{sec:speculation} speculates wildly about the future of DL systems more generally.

%the DL literature is rich in studies that investigate scaling DL models across various dimensions\cite{kaplan2020scaling,you2017large,you2020large}

\section{Background}\label{sec:background}
What are the design choices made by DL framework architects and what are their costs?
That is to say, what are the costs of the abstractions

These frameworks encapsulate and "abstract away" many of the implementation details of DL, such as
\begin{itemize}
  \item building the dataflow graph between units/layers (and the corresponding gradient-flow graph)
  \item tensor manipulation and memory layout
  \item hardware specific optimizations
\end{itemize}

\subsection{GPUs}\label{subsec:gpus}

\subsection{Graph compilers}\label{subsec:graph-compilers}

\subsubsection{Static}

\subsubsection{Dynamic}

\section{Methodology}\label{sec:methodology}

\begin{figure}
  \includegraphics[width=.7\linewidth]{plots/resnet50.png}\label{fig:figure5}
\end{figure}

We implement ResNet-50 at four levels of abstraction: PyTorch, TorchScript, LibTorch, and cuDNN.
The reason for staying within the same ecosystem (PyTorch)is, in theory, we keep as many of the pieces of functionality orthogonal to our concerns as possible.
We'll see that that reasoning doesn't quite bear out (see\ref{sec:discussion}).

\subsection{Implementations}\label{subsec:implementations}

\subsection{Profiling}\label{subsec:profiling}

\section{Results}\label{sec:results}

\subsection{Training and evaluation}\label{subsec:training-and-evaluation}

\subsection{Memory and utilization}\label{subsec:memory-and-utilization}

\section{Discussion}\label{sec:discussion}

% initialization in conv layers in pytorch/libtorch
\section{Future work}\label{sec:futurework}
\section{Speculation}\label{sec:speculation}

%%% The next two lines define the bibliography style to be used, and
%%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\appendix
\appendixpage
\addappheadtotoc

\section{Appendix}\label{sec:appendix}

%\input{plots/3dplots.tex}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
